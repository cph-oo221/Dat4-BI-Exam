{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a97005-8f96-4fcd-bd76-456b5e5074ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data representation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as gph\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "import graphviz\n",
    "\n",
    "# Machine learning\n",
    "import scipy.cluster.hierarchy as ch\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import  KMeans, MeanShift, estimate_bandwidth, AgglomerativeClustering\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Train and Testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Save and get model(s)\n",
    "import pickle\n",
    "\n",
    "# Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5be21b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we will apply different stastistical methods, to try and analyse patterns and correlation in different lifestyle factors. This will involve trying to train different machine learning models, to see how precisely these patterns can be used to predict how much carbon is emitted based on the correlating factors.\n",
    "We will try to answer the following questions: \n",
    "\n",
    "- Can an individuals carbon emissions be predicted by their lifestyle factors?\n",
    "- Which lifestyle factor have the biggest impact on carbon emissions?\n",
    "- How can this information be used by individuals and the society for reducing carbon emissions?\n",
    "\n",
    "#### We have formulated the following hypothesises, to help us answer these questions:\n",
    "\n",
    "H0 = an individuals carbon emissions have no relation to their lifestyle factors\n",
    "\n",
    "H1 = an individuals carbon emissions can be predicted by their lifestyle factors\n",
    "\n",
    "H2 = The most important factor for an individuals carbon emission is their style of transport\n",
    "\n",
    "H3 = The information have clear tendencies, and can make it easy for people to understand how their lifestyle\n",
    "affects their carbon emissions\n",
    "\n",
    "H4 = Society can rely on the accuracy of this information for encouraging and/or discouraging behaviours of\n",
    "individuals\n",
    "\n",
    "H5 = Time spent on the internet has a significant impact on an individuals carbon emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8460b-e52e-4518-9e66-8e87b499d2b9",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc728a7-5c46-4cf5-8e40-373c61233320",
   "metadata": {},
   "source": [
    "### Data Collection / loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cee6b5-9a4f-4a73-8c1a-47f7f9cb1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from csv into pandas dataframe:\n",
    "df = pd.read_csv('./data/Carbon-Emission.csv', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b048ee-855d-480b-9301-ba2f1db34ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the size of the dataframe:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143cd251-294e-4a37-a2da-19a7d48fdedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[7511]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e8a75-47dd-4a3e-90aa-2a76293180fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad7600-eda7-4e69-a398-372c1f442260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values:\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97dc23-f1a5-48bb-aeb8-5bee18dccb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing nan values with None, this we do beacuse we know/can see that pepole that dont drive has nan values as vehicle type\n",
    "df = df.replace(np.nan, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e6cbd-8a2a-4512-9894-382aeaead63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that missing values have been replaced:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf74406-3cc5-43b1-9ddf-ddb8c6cbf2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some insights of the value scope:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f909c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chek numeric values for distribution\n",
    "num_cols = df.select_dtypes(include=['int64']).columns\n",
    "df_nums = df[num_cols]\n",
    "df_nums.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the numeric values:\n",
    "df_nums.hist(figsize=(10, 10), bins=50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the distribution of the target variable:\n",
    "df_nums['CarbonEmission'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency chart for columns:\n",
    "selected_columns = df.select_dtypes(include=['object']).columns[: -2]\n",
    "\n",
    "frequency_count_list = {}\n",
    "\n",
    "for col in selected_columns:\n",
    "    counts = df[col].value_counts()\n",
    "    frequency_count_list[col] = counts\n",
    "\n",
    "for column, counts in frequency_count_list.items(): \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    counts.plot(kind='bar')\n",
    "    plt.title(f\"Frequency chart for column: {column}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    for i, v in enumerate(counts):\n",
    "        plt.text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7653b4",
   "metadata": {},
   "source": [
    "### Data cleaning and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8173d2",
   "metadata": {},
   "source": [
    "It looks like it follows a kind of skewed normal distribution, with a few outliers lying greatly above the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ea744-4263-4e0c-9fee-db1f40b6df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode/transform data numeric\n",
    "encoder = OrdinalEncoder(dtype=np.int64)\n",
    "\n",
    "# Making copy of df to a new dataframe called: df_numeric \n",
    "df_numeric = df.copy()\n",
    "\n",
    "# Gettning all columns that has object type:\n",
    "cate_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_numeric[cate_columns] = encoder.fit_transform(df[cate_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bc1a8-65d3-42f9-8fa8-818ad55353e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that cols have been encoded:\n",
    "df_numeric.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aaa03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba549c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Label encoding\n",
    "with open('models/encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b3aca-8700-489f-a563-336aeae1101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_numeric.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c017cdb-d00f-423d-8510-91cdcaad99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap that can show correlation\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".1f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8703c",
   "metadata": {},
   "source": [
    "The biggest correlation with carbon emissions, seem to be the monthly travelled distance by personal vehicle. Other factor with clear correlations seem to be type of transport, vehicle type and frequency of travelling with airplane. There also seem to be a smaller correlation with the amount of clothes people buy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Vehicle Monthly Distance Km')\n",
    "plt.ylabel('Carbon Emissions')\n",
    "plt.scatter(df_numeric['Vehicle Monthly Distance Km'], df_numeric['CarbonEmission'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec8599",
   "metadata": {},
   "source": [
    "Does internet usage have an impact on carbon emissions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55993e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['CarbonEmission']['How Long Internet Daily Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64380892",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('How Long Internet Daily Hour')\n",
    "plt.ylabel('Carbon Emissions')\n",
    "plt.bar(df_numeric['How Long Internet Daily Hour'], df_numeric['CarbonEmission'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca25ac0",
   "metadata": {},
   "source": [
    "There is a slight correlation, but nowhere near enough to asser any tendencies. We are lucky computer people!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afca39a",
   "metadata": {},
   "source": [
    "Let's see if theres as difference between male an female patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe based on sex, and see if theres a difference in the correlation\n",
    "df_male = df_numeric[df_numeric['Sex'] == 1]\n",
    "df_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c602b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female = df_numeric[df_numeric['Sex'] == 0]\n",
    "df_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male_corr = df_male.corr()\n",
    "df_female_corr = df_female.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_male_corr, annot=True, fmt=\".1f\")\n",
    "plt.title('Male correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_female_corr, annot=True, fmt=\".1f\")\n",
    "plt.title('Female correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c08d19",
   "metadata": {},
   "source": [
    "There doesn't really seem to be a difference of how much the factors weigh between males and females"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b1361",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650d72b-314d-4633-aac3-314b29e43d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.plot.box(rot=90, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e53d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nums['Vehicle Monthly Distance Km'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daaa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "med = df_nums['Vehicle Monthly Distance Km'].median()\n",
    "df_nums['Vehicle Monthly Distance Km'].max() - med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d11051",
   "metadata": {},
   "source": [
    "As we can see, the distribution of distance travelled by vehicle is very skewed, with 75% of the data lying beneath 3000 km pr month, with quite a few outlier being outragerously above the median. This might still be an accurate representation though, since some individuals just might travel that much. \n",
    "We'll keep it for the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b04964",
   "metadata": {},
   "source": [
    "Let's see if people are as accurate as they think, when they say they're energy efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_numeric.iloc[[6550]]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.groupby('Energy efficiency')\n",
    "\n",
    "grouped = df.groupby('Energy efficiency')['CarbonEmission'].agg(['mean', 'sum']).reset_index()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Define the positions for the bars\n",
    "index = np.arange(len(grouped['Energy efficiency']))\n",
    "\n",
    "# Create side-by-side bars for 'Sum' and 'Mean'\n",
    "plt.bar(index, grouped['mean'], bar_width, color='red')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Mean Carbon Emissions')\n",
    "plt.title('Mean by Energy efficiency')\n",
    "plt.xticks(index + bar_width / 2, grouped['Energy efficiency'])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['sum'] = grouped['sum'].astype(float)\n",
    "grouped.plot.bar(x='Energy efficiency', y='sum', figsize=(8, 6), rot=0, title='Sum by Energy efficiency (values in millions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abea50",
   "metadata": {},
   "source": [
    "We can see that people say that the are energy efficient generally tend to have a little bit lower carbon emission in total, but the difference really is only about 1,5% lower than the 'No' category.\n",
    "\n",
    "The total sum of carbon emissions seems to be higher by people declaring that they're being energy efficient, indicating that a lot more people think they are being efficient than not, while not really making any difference in the emitted carbon dioxide!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76d49",
   "metadata": {},
   "source": [
    "### Plane travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87abad56",
   "metadata": {},
   "source": [
    "We can see theres a small correlation between the Frequency of air travel, and their carbon emission, let's sxplore what it means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_air = df.groupby('Frequency of Traveling by Air').size().reset_index(name='Count')\n",
    "group_air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the frequency\n",
    "group_air.plot.pie(y='Count', labels=['Never', 'Rare', 'Frequently', 'Very Frequently'], autopct='%1.1f%%', figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('Frequency of Traveling by Air')['CarbonEmission'].agg(['mean', 'sum']).reset_index()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97955c",
   "metadata": {},
   "source": [
    "Total mean = 2262.8\n",
    "\n",
    "total mean / very frequently = 0.75\n",
    "\n",
    "Very frequently travelers emit around 25% on average than the average across all flight travel categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2525ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.plot.bar(x='Frequency of Traveling by Air', y=['mean'], figsize=(8, 6), rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b28d6e",
   "metadata": {},
   "source": [
    "As we can see, the distribution of people is fairly equal between all different categories of flight frequency, but there's a clearly tendency of higher average carbon emissions from people who are in the 'very frequent' category.\n",
    "\n",
    "The mean carbon emssions are around 22% higher for people who travel very frequently by air.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc5bff",
   "metadata": {},
   "source": [
    "## Supervised Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6ef33",
   "metadata": {},
   "source": [
    "Let's try and see if we can predict total carbon emission values by training different classifier models.\n",
    "Since we're doing classification, we'll bin the carbon emission numbers into categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed529296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91245ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = df_numeric[['CarbonEmission']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrans = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "scaled_data['CarbonEmission_trans_norm'] = qtrans.fit_transform(scaled_data[['CarbonEmission']])\n",
    "\n",
    "print('Mean:', scaled_data['CarbonEmission_trans_norm'].mean())\n",
    "print('Standard Deviation:', scaled_data['CarbonEmission_trans_norm'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = df_numeric.copy().drop(columns=['CarbonEmission'], axis=1)\n",
    "df_bin = pd.concat([df_part, scaled_data['CarbonEmission_trans_norm']], axis=1)\n",
    "df_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_max = df_bin['CarbonEmission_trans_norm'].max()\n",
    "emission_min = df_bin['CarbonEmission_trans_norm'].min()\n",
    "\n",
    "bins = np.linspace(emission_min -1, emission_max, 5)\n",
    "\n",
    "df_bin['EmissionBin'] = pd.qcut(df_bin['CarbonEmission_trans_norm'], q=4, labels=False)\n",
    "df_bin.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_bin = df_bin.groupby('EmissionBin').size().reset_index(name='Count')\n",
    "grouped_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.drop('CarbonEmission_trans_norm', axis=1, inplace=True)\n",
    "df_bin.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0790a2",
   "metadata": {},
   "source": [
    "## Prediction by classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd046f9",
   "metadata": {},
   "source": [
    "#### Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bdbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb894a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into array\n",
    "array = df_bin.values\n",
    "# Create two axis for the test data\n",
    "X = array[:,0:19] \n",
    "Y = array[:,-1]   # CarbonEmission\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59744152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split proportion\n",
    "test_set_size = 0.15\n",
    "\n",
    "# Initial value for randomization\n",
    "seed = 42 # this is the answer\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_set_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed39cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GaussianNB for numeric data\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cddae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "nb.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6f99b",
   "metadata": {},
   "source": [
    "An accuracy of about 70 percent, not bad for a first try, but I think we can do better.\n",
    "Let's try to focus on the correlating values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f47e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indexes = ['Body Type', 'Diet', 'How Often Shower', 'Heating Energy Source',\n",
    "       'Transport', 'Social Activity', 'Monthly Grocery Bill',\n",
    "       'Frequency of Traveling by Air', 'Vehicle Monthly Distance Km',\n",
    "       'Waste Bag Size', 'Waste Bag Weekly Count',\n",
    "       'How Many New Clothes Monthly'] # Columns with correlation > 0.1\n",
    "df_features = df_numeric[column_indexes]\n",
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efcb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_numeric[column_indexes].values\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc727f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_set_size, random_state=seed)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a4477",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nb.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a41f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae640f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(Y_test, prediction)\n",
    "print(cmat)\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c4916",
   "metadata": {},
   "source": [
    "In the end, it made a bigger difference to adjust the test set size to around 15% of the set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc52ea",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 0.15\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_set_size, random_state=seed)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Decision Trees Classifier \n",
    "params = {'max_depth': 6}\n",
    "treeM = DecisionTreeClassifier(**params)\n",
    "treeM.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_data = tree.export_graphviz(treeM, out_file=None, \n",
    "                         feature_names=df_features.columns, class_names = True,        \n",
    "                         filled=True, rounded=True, proportion = False, special_characters=True)  \n",
    "dtree = graphviz.Source(gr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render the tree as pdf\n",
    "dtree.render('dtree_render', format='pdf', cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8bf4c2",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2788dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = treeM.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(Y_test, y_predict)\n",
    "print(cmat)\n",
    "print(classification_report(Y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa59e34",
   "metadata": {},
   "source": [
    "Even better than before. Maybe our predictions can be even more accurate by using several decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174e64d",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 0.15\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_set_size, random_state=seed)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 300, max_depth = 13)\n",
    "forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4e113",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = forest.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(Y_test, y_predict)\n",
    "print(cmat)\n",
    "print(classification_report(Y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(forest, open('./models/forest_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50554b",
   "metadata": {},
   "source": [
    "### Classification summary\n",
    "\n",
    "**Iteration 1**\n",
    "- No data normalization\n",
    "- 5 equally divided bins <br />\n",
    "\n",
    "80% accuracy (RFC)\n",
    "\n",
    "At 80% accuracy the random forest classifier seems to be the most accurate model we have tested. It seems to correctly categorize a lot of the lower emitting categories, and is a little less accurate on cases with fewer instances. It's still making some big mistakes which might not show up in the overall calculated score, but is very significant.\n",
    "For example, none of the models have succesfully caught any people in the topmost carbon emitting category. Perhaps implying, that the category data sould be binned differently, becuase if only very few instances fit into the bin, the model might not have any chance of recieving enough training data to be able to recognize the extreme cases.\n",
    "\n",
    "**Iteration 2**\n",
    "- Data transformed to normal distribution\n",
    "- 4 equally divided bins\n",
    "\n",
    "87% accuracy (RFC).\n",
    "\n",
    "Model accuracy have improved in accuracy. I accredit this to the fewer bins and the now normal distribution of the data. Even though the accuracy seems higher, the result is still unsatisfactory since it fails to identify any extreme cases in both high and low end. Because of the normal distribution transform, the data have accumulated around the mean, resulting in only around 2% of the data fitting in the extreme bins. The training data might still be to sparse to let the model identify these few extreme cases\n",
    "\n",
    "**Iteration 3**\n",
    "- Data transformed to normal distribution\n",
    "- 3 equally divided bins\n",
    "\n",
    "92% accuracy (RFC)\n",
    "\n",
    "Model have once model improved in accuracy. Even with only 3 bins, the middle bin is responsible for 92% of the data. The fewer amount of bins, gives better results when identifying outlier cases, but it is still very overfitted and not very useful\n",
    "\n",
    "**Iteration 4**\n",
    "- Data transformed to normal distribution\n",
    "- 4 bins divided by quantiles\n",
    "\n",
    "67% accuracy (RFC)\n",
    "\n",
    "Accuracy has dropped to 67%, but seems to catch a lot more of the extreme cases, now the bin sizes have been adjusted. It now represents the data tendencies way better, and are less overfitted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ca44f",
   "metadata": {},
   "source": [
    "## Prediction by regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7d2e4",
   "metadata": {},
   "source": [
    "### XGBregressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a31ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features (x) and the target (y)\n",
    "ml_x = df_numeric.drop('CarbonEmission', axis=1).values\n",
    "ml_y = df_numeric['CarbonEmission'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ml_x, ml_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91900ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model(s)\n",
    "xgbr = XGBRegressor()\n",
    "linearreg = LinearRegression()\n",
    "svr = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01517737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model(s)\n",
    "xgbr.fit(X_train, y_train)\n",
    "linearreg.fit(X_train, y_train)\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target\n",
    "y_pred_xgbr = xgbr.predict(X_test)\n",
    "y_pred_linearreg = linearreg.predict(X_test)\n",
    "y_pred_svr = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf67af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_res = {\n",
    "    \"ML models\": [\"XGBRegressor\", \"LinearRegression\", \"SVR\"],\n",
    "    \"r2\": [\n",
    "        r2_score(y_test, y_pred_xgbr),\n",
    "        r2_score(y_test, y_pred_linearreg),\n",
    "        r2_score(y_test, y_pred_svr)\n",
    "    ],\n",
    "    \"MAE\": [\n",
    "        mean_absolute_error(y_test, y_pred_xgbr),\n",
    "        mean_absolute_error(y_test, y_pred_linearreg),\n",
    "        mean_absolute_error(y_test, y_pred_svr)\n",
    "    ]\n",
    "}\n",
    "# Create a dataframe\n",
    "ml_res_df = pd.DataFrame(ml_res)\n",
    "ml_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be445604",
   "metadata": {},
   "source": [
    "The XGBoost model looks to have the best accuracy, with a score of 0.9747 on the test set, with only a MAE of 123.98. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78975915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing xgboost model\n",
    "pickle.dump(xgbr, open('./models/xgboost_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test, y_pred_xgbr, 'o')\n",
    "plt.plot(y_test, y_test, 'r')\n",
    "plt.title(\"XGBRegressor\")\n",
    "plt.xlabel(\"Actual values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4987f",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_x = df_numeric.drop('CarbonEmission', axis=1).columns\n",
    "ml_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_x = df_numeric.drop('CarbonEmission', axis=1).values\n",
    "ml_y = df_numeric['CarbonEmission'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y = ml_y\n",
    "x = scaler.fit_transform(ml_x)\n",
    "print(x)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb874b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler\n",
    "with open('models/scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ad331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for neural network:\n",
    "# Selecting input shape, and defining number of layers & neurons\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(19,)),             # 19 feature / input neurons\n",
    "    \n",
    "    keras.layers.Dense(256, activation='relu'),  \n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "\n",
    "    keras.layers.Dense(1)                        # Output layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f80ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e17b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d53379",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {t_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33316586",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('./models/nn_model.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d18a5",
   "metadata": {},
   "source": [
    "### Neural Network Results\n",
    "\n",
    " [x1,... xn] means the numbers of neurons for each layer in the network\n",
    "\n",
    "<table><tr>\n",
    "<td>\n",
    "\n",
    "|Layers                      | Epochs | Accuracy | Test Loss | MAE     |\n",
    "|----------------------------| -------| -------- | ----------| --------|\n",
    "|1 lay [32]                  | 50     | 0.6514   | 485.4101  | 488.3452|\n",
    "|1 lay [64]                  | 50     | 0.6794   | 458.6350  | 457.2778|\n",
    "|1 lay [128]                 | 50     | 0.7634   | 360.0965  | 358.2312|\n",
    "|1 lay [256]                 | 50     | 0.8073   | 314.0002  | 303.8010|\n",
    "|1 lay [512]                 | 50     | 0.8387   | 277.7811  | 267.3893|\n",
    "\n",
    "\n",
    "</td><td>\n",
    "\n",
    "|Layers                      | Epochs | Accuracy | Test Loss | MAE     |\n",
    "|----------------------------| -------| -------- | ----------| --------|\n",
    "|2 lay [32, 64]              | 50     | 0.8260   | 305.7189  | 287.4525|\n",
    "|2 lay [64, 128]             | 50     | 0.8519   | 280.6377  | 259.1389|\n",
    "|2 lay [128, 256]            | 50     | 0.9222   | 195.7561  | 177.1057|\n",
    "|2 lay [256, 512]            | 50     | 0.9588   | 154.8777  | 133.3263|\n",
    "\n",
    "</td></tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "|Layers                      | Epochs | Accuracy | Test Loss | MAE     |\n",
    "|----------------------------| -------| -------- | ----------| --------|\n",
    "|3 lay [32, 64, 128]         | 50     | 0.8919   | 219.7377  | 210.3670|\n",
    "|3 lay [64, 128, 256]        | 50     | 0.9469   | 167.5058  | 152.9221|\n",
    "|3 lay [128, 256, 512]       | 50     | 0.9755   | 126.4031  | 104.1510|\n",
    "|3 lay [256, 512, 512]       | 50     | 0.9800   | 115.4065  | 81.7558 |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "|Layers                      | Epochs | Accuracy | Test Loss | MAE     |\n",
    "|----------------------------| -------| -------- | ----------| --------|\n",
    "|4 lay [32, 64, 128, 256]    | 50     | 0.9787   | 117.9240  | 108.2563|\n",
    "|4 lay [64, 128, 256, 512]   | 50     | 0.9792   | 117.7210  | 94.6650 |\n",
    "|4 lay [128, 256, 512, 512]  | 50     | 0.9713   | 134.8475  | 80.8660 |\n",
    "|4 lay [256, 512, 512, 512]  | 50     | 0.9819   | 109.2275  | 75.5905 |\n",
    "\n",
    "</td></tr></table>\n",
    "\n",
    "<!-- |19 lay [all 32] | 50     | 0.974    | 126.68 | v| -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2a8fb",
   "metadata": {},
   "source": [
    "NB: Source used for helping to make this neural network: [Neural Network Regression Implementation and Visualization in Python](https://medium.com/@nandiniverma78988/neural-network-regression-implementation-and-visualization-in-python-d5893713ed79)\n",
    "\n",
    "#### Approach (simple model)\n",
    "\n",
    "The approach was to start with a simple model, and then add complexity over time to try to increase the accuracy of the model. At the beginning, we chose to start with only one hidden layer and 32 neurons, and then each time increase neurons times 2. We made the assumption that the max number of neurons in a layer should be 512 to avoid overfitting, beacuse we know that more neurons means that the model has a higher capacity to learn form the traning data; however we dont want the model to learn from noise and other irrelevant factors, which can lead to overfitting.\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "The 4 performance metrics of the neural network with varying configurations over 50 epochs of layers and neurons are shown in the table above. This config are defined by the number of layers and the number of neurons in each layer. For all iterations, we chose the activations function: ReLU:\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "#### Results\n",
    "\n",
    "The results show that the accuracy of the model increases with the number of layers and neurons. We can see that the accuracy of the model is 0.9819 with 4 layers (256, 512, 512, 512 neurons) The test loss is 109.2275 and the mean absolute error is 75.5905. This is the best result we have achieved with the neural network.\n",
    "We can also conclude that the neural network has better results the our XGBoost model, that 'only' got an accuracy of 0.9747 and a MAE of 123.9864.\n",
    "\n",
    "\n",
    "#### Alterantive improvements\n",
    "\n",
    "Without a doubt, we know that our neural network has a good accuracy 0.9819 and a littel MAE on 75.5905. However, we can still alot of different was to try and improve the model.\n",
    "The different tings that can be experimaneted with are the following:\n",
    "- The number of epochs\n",
    "- The number of layers\n",
    "- The number of neurons in each layer\n",
    "- The activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57f159",
   "metadata": {},
   "source": [
    "## Unsupervised machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cff292",
   "metadata": {},
   "source": [
    "### Applying the data to K-means, Hierarchical clustering & Mean Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e267022",
   "metadata": {},
   "source": [
    "The perpose of applying these clustering models, is to see if, there are patterns in the data, that we haven't been able to see before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Create an instance of PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_scaled = scaler.fit_transform(df_numeric)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a new DataFrame with the PCA results\n",
    "df_numeric_pca = pd.DataFrame(X_pca, columns=[f'PC{i}' for i in range(1, pca.n_components_ + 1)])\n",
    "df_numeric_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4881681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the first two principal components\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ac023",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_numeric_pca.values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b89fe",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f957b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the dendrogram to find the optimal number of clusters\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "dendogram = ch.dendrogram(ch.linkage(X, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Observations')\n",
    "plt.ylabel('Euclidean distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ec656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering Model\n",
    "n_clusters = 2\n",
    "model = AgglomerativeClustering(n_clusters, affinity = 'euclidean', linkage = 'ward')\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = model.fit_predict(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap='viridis')\n",
    "plt.title('Discovered Clusters')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = model.fit_predict(X)\n",
    "print(\"Hierarchical Silhouette Score:\", silhouette_score(X, hc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2391cb",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84fcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering Model\n",
    "# Extracting the features\n",
    "X_kmeans = X\n",
    "X_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find the optimal number of clusters\n",
    "distortions = []\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k, n_init=10).fit(X_kmeans)\n",
    "    model.fit(X_kmeans)\n",
    "    distortions.append(sum(np.min(cdist(X_kmeans, model.cluster_centers_, 'euclidean'), axis=1)) / X_kmeans.shape[0]) \n",
    "print(\"Distortion: \", distortions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the elbow method\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9735750",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e890b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=k_clusters, n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2357fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score for KMeans clustering for different number of clusters\n",
    "scores = []\n",
    "K = range(2, 10)\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k, n_init=10)\n",
    "    model.fit(X_kmeans)\n",
    "    score = metrics.silhouette_score(X_kmeans, model.labels_, metric='euclidean', sample_size=len(X_kmeans))\n",
    "    print(\"\\nNumber of clusters =\", k)\n",
    "    print(\"Silhouette score =\", score)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57890b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "plt.title('Silhouette Score Method for Discovering the Optimal K')\n",
    "plt.plot(K, scores, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e529d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kmeans.predict(X_kmeans)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d657cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the endividual clusters\n",
    "for i in range(k_clusters):\n",
    "    # slice the cluster\n",
    "    cluster = X_kmeans[y == i]    \n",
    "    # print the shape\n",
    "    print(\"Cluster \", i, \": \", cluster.shape)    \n",
    "    # plot the points of this cluster\n",
    "    plt.scatter(cluster[:, 0], cluster[:, 1])   \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01784cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising all the clusters\n",
    "plt.scatter(X_kmeans[:, 0], X_kmeans[:, 1], c=y, s=20, cmap='viridis')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Silhouette Score\n",
    "visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n",
    "visualizer.fit(X_kmeans)\n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea5fe3",
   "metadata": {},
   "source": [
    "### Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features\n",
    "x_ms = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the bandwidth of the data\n",
    "bandwidth = estimate_bandwidth(x_ms, quantile=0.2, n_samples=200)\n",
    "bandwidth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48262fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the instance of the MeanShift model\n",
    "msmodel = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "msmodel.fit(x_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the labels\n",
    "labels = msmodel.labels_\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = msmodel.cluster_centers_\n",
    "cluster_centers\n",
    "# Predict the cluster for all the samples\n",
    "Y = msmodel.predict(x_ms)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msmodel.labels_)\n",
    "\n",
    "plt.scatter(x_ms[:,0], x_ms[:,1], s=50, c=labels, cmap='viridis')\n",
    "plt.title(f'Estimated number of clusters = {n_clusters_}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# Visualising the clusters in 3D\n",
    "df_3d = pd.DataFrame(x_ms, columns=['x', 'y'])\n",
    "df_3d['label'] = labels\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(df_3d, x='x', y='y', z='label', color='label', symbol='label')\n",
    "fig.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('Discovered Clusters')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_ms[:,0], x_ms[:,1],  marker='o', cmap='viridis', c=labels)\n",
    "ax.scatter(cluster_centers[:,0], cluster_centers[:,1], marker='x', \n",
    "           color='red', s=100, linewidth=3, zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score for MeanShift clustering\n",
    "score = silhouette_score(x_ms, msmodel.labels_, metric='euclidean')\n",
    "print('Silhouette Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c1e55",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af840d",
   "metadata": {},
   "source": [
    "Theres much interesing information in exploring this dataset, and we can definetely find a lot of patterns that is crucial for identifying which personal lifestyle factors are significant when it comes to calculationg our total carbon emission output.\n",
    "\n",
    "**Here is our discoveries summarized:**\n",
    "\n",
    "- The most significant factors for an individuals carbon emission seem to be their choice of transport. People that drive many kilometers monthly in personal vehicles are also the one who have a tendency to have the highest carbon emissions.\n",
    "\n",
    "- Theres a small indication that internet usage has an impact on carbon emissions to correlating factors, but no direct pattern can be concluded, so we deem it as insignificant\n",
    "\n",
    "- There seem to be very little difference in the correlating factors between males and females\n",
    "\n",
    "- People who see themselves as 'Energy efficient' only seem the emit about 1,5% less carbon than people who say they're 'Not energy efficient', which we conclude makes it insignificant\n",
    "\n",
    "- People who travel with airplanes very frequently seem to emit around 22% more carbon on average, than people who never fly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e9a63",
   "metadata": {},
   "source": [
    "#### Prediction by classification\n",
    "We tried to use different classification machine learning methods to try and predict carbon output based on correlating factors, and clustering methods for trying to discov hidden patterns in the data.\n",
    "\n",
    "For classification we tried:\n",
    "\n",
    "- Naive Bayes classifier\n",
    "- Decision Tree classifier\n",
    "- Random Forest classifier\n",
    "\n",
    "We tried several iterations for optimizing the models by adjusting the category amount and the normalisation of the input data. The RFC model was the most succesful, with an accuracy of aaround 92% with 3 categories. However, we deem the result as unsaticfactory, since the input data still is a bit unsuited for this kind of machine learning, with 92% of the data fitting into a single category. The good accuracy is due to overfitting.\n",
    "\n",
    "On the last iteration we tried to divide the categories by quantiles, which made the model drop to 67% accuracy, but is a way more useful and less overfitted model.\n",
    "We therefore conclude the best classification model to be the RFC model with training data binned into 4 categories based on the quartiles of the data, since it could make decently accurate predictions, and represent the tendencies in the data without being overfitted to the training data. However, this kind of model is definetely unsuited for this task. Even though we got satisfactory predictions, the information value of the predictions are very low, since the binning of the results into categories, makes the prediction results vague and without meaning.\n",
    "\n",
    "#### Prediction by regression\n",
    "\n",
    "Since we are trying to predict numeric values, we thought might prove more valuable to try and predict actual carbon emission values by using regression models.\n",
    "This turned out to be a way better fit than the classification models, both in accuracy and in the information value of the output.\n",
    "\n",
    "we used an advanced gradient boosting model for this purpose\n",
    "in the form of an XGBRegressor.\n",
    "\n",
    "It was quick to train the model, and\n",
    "we ended up getting very good results,\n",
    "wtih an accuracy of 97% on the test set.\n",
    "\n",
    "A good fit for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ab937",
   "metadata": {},
   "source": [
    "#### Clustering summary:\n",
    "\n",
    "We have try using K-means, Hierarchical clustering & Mean shift to see if there are any patterns in the data, but it seems that there are no clear patterns in the data, when applying these clustering methods to the hole dataset. We have tried with and without scaling the data. Additionally, we have tried to use different numbers of clusters, but the results are still the same. \n",
    "This is concluded based on the visualisation of the clusters from the different methods due to the visualistations being poor; However, the silhouette score have actually been quite high, which indicates that there are some patterns in the data.\n",
    "\n",
    "Silhouette score (without scaling and pca):\n",
    "* K-means: 0.727 (with 2 clusters), 0.624 (with 3 clusters)\n",
    "* Hierarchical clustering: 0.426\n",
    "* Mean shift: 0.727\n",
    "\n",
    "When applying scaling and pca to the data, the visualisation of the clusters are much better; however, the silhouette score is lower than without scaling and pca:\n",
    "\n",
    "Silhouette score (with scaling and pca):\n",
    "* K-means: 0.549 (2 clusters), 0.426 (3 clusters)\n",
    "* Hierarchical clustering: 0.542 (2 clusters)\n",
    "* Mean shift: 0.546 (2 clusters)\n",
    "\n",
    "We have also tried using only some parts of the datas features, these results are gives som visualisation endication of patterns.\n",
    "The features used for the clustering are: 'Monthly Grocery Bill', 'Waste Bag Weekly Count', 'How Long TV PC Daily Hour', 'How Many New Clothes Monthly', 'How Long Internet Daily Hour'\n",
    "\n",
    "Silhouette score (without scaling and pca):\n",
    "* K-means: 0.79 (with 2 clusters), 0.727 (with 3 clusters)\n",
    "* Hierarchical clustering: 0.698\n",
    "* Mean shift: 0.597\n",
    "\n",
    "Other features combinations have also been tried, but the results gives similar visualisation and silhouette score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
